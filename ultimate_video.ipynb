{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxmetLVsy6Bn",
        "outputId": "8dcee24a-f7ef-462a-dbd8-5d861d7bab13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fer\n",
            "  Downloading fer-22.5.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fer) (3.7.1)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from fer) (4.7.0.72)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fer) (2.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fer) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fer) (2.27.1)\n",
            "Collecting facenet-pytorch (from fer)\n",
            "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from fer) (4.65.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from fer) (1.0.3)\n",
            "Collecting ffmpeg==1.4 (from fer)\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fer) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch->fer) (1.22.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch->fer) (0.15.2+cu118)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fer) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fer) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fer) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fer) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fer) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fer) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fer) (2.8.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->fer) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->fer) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->fer) (2.25.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->fer) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fer) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fer) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->fer) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fer) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fer) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->fer) (1.16.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch->fer) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch->fer) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch->fer) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch->fer) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch->fer) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch->fer) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch->fer) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch->fer) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch->fer) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision->facenet-pytorch->fer) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision->facenet-pytorch->fer) (1.3.0)\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=c12c0ed56359d2ea3445857f42c665dc70ee92860be560da45f9156b7dbdf5da\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg, facenet-pytorch, fer\n",
            "Successfully installed facenet-pytorch-2.5.3 fer-22.5.1 ffmpeg-1.4\n"
          ]
        }
      ],
      "source": [
        "pip install fer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwIieME_yO9Z",
        "outputId": "9e63dcd8-b116-493b-c80e-9bf52fa95f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/6-10\n"
          ]
        }
      ],
      "source": [
        "from fer import Video\n",
        "from fer import FER\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/6-10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5NnPD_Ah1GEo"
      },
      "outputs": [],
      "source": [
        "#cd sample_data\n",
        "#ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-W-PG_tzORB",
        "outputId": "26842d8d-3b20-4d84-b4bc-75609aa79288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UDVi1ozdzHag"
      },
      "outputs": [],
      "source": [
        "# Put in the location of the video file that has to be processed\n",
        "file_name = \"/content/6-10/MyDrive/11-32/c5_2.mp4\"\n",
        "location_videofile = file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wqzM7HlOzn9K"
      },
      "outputs": [],
      "source": [
        "# Build the Face detection detector\n",
        "face_detector = FER(mtcnn=True)\n",
        "# Input the video for processing\n",
        "input_video = Video(location_videofile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_b2f6zmz4_e",
        "outputId": "2c54f889-8269-4b09-991a-5b1a7ffbac43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fer:30.00 fps, 9000 frames, 300.05 seconds\n",
            "INFO:fer:Making directories at output\n",
            " 19%|█▊        | 1671/9000 [05:37<23:29,  5.20frames/s]"
          ]
        }
      ],
      "source": [
        "# The Analyze() function will run analysis on every frame of the input video.\n",
        "# It will create a rectangular box around every image and show the emotion values next to that.\n",
        "# Finally, the method will publish a new video that will have a box around the face of the human with live emotion values.\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  processing_data = input_video.analyze(face_detector, display=False, frequency=2.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfWO88upk4EG"
      },
      "outputs": [],
      "source": [
        "# We will now convert the analysed information into a dataframe.\n",
        "# This will help us import the data as a .CSV file to perform analysis over it later\n",
        "vid_df = input_video.to_pandas(processing_data)\n",
        "#vid_df = input_video.get_first_face(vid_df)\n",
        "vid_df = input_video.get_emotions(vid_df)\n",
        "vid_df.to_csv('/content/6-10/MyDrive/11-32/output/df_c5_2.csv', index=False)\n",
        "vid_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqj2RfKuk8k5"
      },
      "outputs": [],
      "source": [
        "# Plotting the emotions against time in the video\n",
        "#vid_df = pd.read_csv(\"/content/6-10/MyDrive/6-10/df_c21.csv\")\n",
        "pltfig = vid_df.plot(figsize=(50, 10), fontsize=16).get_figure()\n",
        "pltfig.savefig('/content/6-10/MyDrive/11-32/output/emotions_timeline_c5_2.png', dpi=pltfig.dpi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld51Q18clBnB"
      },
      "outputs": [],
      "source": [
        "# We will now work on the dataframe to extract which emotion was prominent in the video\n",
        "angry = sum(vid_df.angry)\n",
        "disgust = sum(vid_df.disgust)\n",
        "fear = sum(vid_df.fear)\n",
        "happy = sum(vid_df.happy)\n",
        "sad = sum(vid_df.sad)\n",
        "surprise = sum(vid_df.surprise)\n",
        "neutral = sum(vid_df.neutral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws12a5gzlRK9"
      },
      "outputs": [],
      "source": [
        "emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "emotions_values = [angry, disgust, fear, happy, sad, surprise, neutral]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pakfy1D6lWQV"
      },
      "outputs": [],
      "source": [
        "score_comparisons = pd.DataFrame(emotions, columns = ['Human Emotions'])\n",
        "score_comparisons['Emotion Value from the Video'] = emotions_values\n",
        "score_comparisons.to_csv(\"/content/6-10/MyDrive/11-32/output/score_comparisons_c5_2\", sep=',', index=False)\n",
        "score_comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maBOv3hcUO_W"
      },
      "outputs": [],
      "source": [
        "'''from multiprocessing import Process\n",
        "\n",
        "def print_func(filename = '/content/6-10/MyDrive/6-10/konventiert/c2_1.mp4'):\n",
        "    # Put in the location of the video file that has to be processed\n",
        "    file_name = name\n",
        "    location_videofile = file_name\n",
        "\n",
        "    # Build the Face detection detector\n",
        "    face_detector = FER(mtcnn=True)\n",
        "    # Input the video for processing\n",
        "    input_video = Video(location_videofile)\n",
        "\n",
        "    # The Analyze() function will run analysis on every frame of the input video.\n",
        "    # It will create a rectangular box around every image and show the emotion values next to that.\n",
        "    # Finally, the method will publish a new video that will have a box around the face of the human with live emotion values.\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      string = name + \"processing_data\"\n",
        "      processing_data = input_video.analyze(face_detector, display=False, frequency=2)\n",
        "\n",
        "    # We will now convert the analysed information into a dataframe.\n",
        "    # This will help us import the data as a .CSV file to perform analysis over it later\n",
        "    vid_df = input_video.to_pandas(processing_data)\n",
        "    vid_df = input_video.get_first_face(vid_df)\n",
        "    vid_df = input_video.get_emotions(vid_df)\n",
        "\n",
        "    # Plotting the emotions against time in the video\n",
        "    pltfig = vid_df.plot(figsize=(20, 8), fontsize=16).get_figure()\n",
        "    fil = 'emotions_timeline'+ name + '.png'\n",
        "    pltfig.savefig(fil, dpi=pltfig.dpi)\n",
        "\n",
        "    # We will now work on the dataframe to extract which emotion was prominent in the video\n",
        "    angry = sum(vid_df.angry)\n",
        "    disgust = sum(vid_df.disgust)\n",
        "    fear = sum(vid_df.fear)\n",
        "    happy = sum(vid_df.happy)\n",
        "    sad = sum(vid_df.sad)\n",
        "    surprise = sum(vid_df.surprise)\n",
        "    neutral = sum(vid_df.neutral)\n",
        "\n",
        "    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "    emotions_values = [angry, disgust, fear, happy, sad, surprise, neutral]\n",
        "\n",
        "    score_comparisons = pd.DataFrame(emotions, columns = ['Human Emotions'])\n",
        "    score_comparisons['Emotion Value from the Video'] = emotions_values\n",
        "    fil = \"score_comparison\"+ name\n",
        "    score_comparisons.to_csv(fil, sep=',', index=False, encoding='utf-8')\n",
        "    score_comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUpMUghPngv7"
      },
      "outputs": [],
      "source": [
        "'''if __name__ == \"__main__\":\n",
        "    names = [ '/content/6-10/MyDrive/6-10/konventiert/c2_2.mp4', '/content/6-10/MyDrive/6-10/konventiert/c2_3.mp4', '/content/6-10/MyDrive/6-10/konventiert/c2_4.pv4']\n",
        "    procs = []\n",
        "    proc = Process(target=print_func)  # instantiating without any argument\n",
        "    procs.append(proc)\n",
        "    proc.start()\n",
        "\n",
        "    # instantiating process with arguments\n",
        "    for name in names:\n",
        "        # print(name)\n",
        "        proc = Process(target=print_func, args=(name,))\n",
        "        procs.append(proc)\n",
        "        proc.start()\n",
        "\n",
        "    # complete the processes\n",
        "    for proc in procs:\n",
        "        proc.join()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}